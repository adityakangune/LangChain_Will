{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1utS7UZ7kwD55l7qKzU1rusgPdf7LgkeU",
      "authorship_tag": "ABX9TyMyCDzmXG2yG8fiA9w6M/Kf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityakangune/LangChain_Will/blob/main/LangChain_Wills.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ckjXVV3KEwqX"
      },
      "outputs": [],
      "source": [
        "pdf_path = \"/content/drive/Othercomputers/My Laptop/Purdue University/Semester 4/Interview Prep/wealth.com/sample_will.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain_community"
      ],
      "metadata": {
        "id": "OU90r-ciFFqJ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pypdf"
      ],
      "metadata": {
        "id": "k34fjyeRFXyY"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "uomHbs11FB05"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(pdf_path)\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "id": "WTaJUtpeFEMe"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìÑ Total Pages Loaded:\", len(pages))\n",
        "print(\"üîπ Sample Page Text:\\n\", pages[0].page_content[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtjJp1YCFWMY",
        "outputId": "a3586e17-83a6-401c-a5a9-e624926a98a7"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Total Pages Loaded: 5\n",
            "üîπ Sample Page Text:\n",
            " Last Will and Testament \n",
            "of \n",
            "___________________________________ \n",
            " \n",
            "I, ________________________, resident in the City of ____________________, \n",
            "County of ____________________, State of ____________________, being of sound \n",
            "mind, not acting under duress or undue influence, and fully understanding the nature \n",
            "and extent of all my property and of this disposition thereof, do hereby make, publish, \n",
            "and declare this document to be my Last Will and Testament, and hereby revoke any \n",
            "and all other wills\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the splitter\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "# ‚úÖ Purpose: We split each page into overlapping 500-character chunks.\n",
        "# üß† Why overlap? To preserve context across sentences that cross chunk boundaries."
      ],
      "metadata": {
        "id": "SZ760RMrFhpT"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "F4k4G4bZFsex"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üß© Total Chunks Created:\", len(chunks))\n",
        "print(\"üìå First Chunk Preview:\\n\", chunks[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A82Pgd3xFoq-",
        "outputId": "406e2454-aada-497b-ee2d-d42b19e5b38a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß© Total Chunks Created: 29\n",
            "üìå First Chunk Preview:\n",
            " Last Will and Testament \n",
            "of \n",
            "___________________________________ \n",
            " \n",
            "I, ________________________, resident in the City of ____________________, \n",
            "County of ____________________, State of ____________________, being of sound \n",
            "mind, not acting under duress or undue influence, and fully understanding the nature \n",
            "and extent of all my property and of this disposition thereof, do hereby make, publish, \n",
            "and declare this document to be my Last Will and Testament, and hereby revoke any\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Embedding"
      ],
      "metadata": {
        "id": "P7y8G3GwH0B_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert each chunk of the will into a numeric vector (embedding)\n",
        "# Store those vectors in a searchable database (FAISS)"
      ],
      "metadata": {
        "id": "-8dXNFFJFqlH"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain_huggingface"
      ],
      "metadata": {
        "id": "QdKzwGDaIRVt"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "# Facebook AI Similarity Search"
      ],
      "metadata": {
        "id": "VR3ViKUgHzfA"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# small transformer model that turns text into 384-dimension vectors.\n",
        "# If two sentences mean similar things, their embeddings will be close together in vector space.\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "sample_vec = embedding_model.embed_query(chunks[0].page_content)\n",
        "print(\"üî¢ Vector length:\", len(sample_vec))\n",
        "print(\"üìä First 5 dims:\", sample_vec[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40ONZksgH-Ck",
        "outputId": "f3d52676-ba60-45c1-f3c7-bad6c6dadda7"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî¢ Vector length: 384\n",
            "üìä First 5 dims: [-0.048935726284980774, 0.141635462641716, 0.023208405822515488, -0.112810418009758, -0.05388515070080757]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install faiss-cpu"
      ],
      "metadata": {
        "id": "Urrd2qOiIrkM"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = FAISS.from_documents(chunks, embedding_model)\n",
        "print(\"‚úÖ FAISS index created with\", len(chunks), \"chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LInJSReIHjv",
        "outputId": "bf4b3f34-b1d5-4e6e-98d3-a8c38fd083dd"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ FAISS index created with 29 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.save_local(\"faiss_index\")\n",
        "print(\"üíæ Saved FAISS index to disk.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emoDsLHEInnG",
        "outputId": "84f13c36-bf04-4892-c0e2-3cb50e84e384"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved FAISS index to disk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Semantic Retrieval"
      ],
      "metadata": {
        "id": "17Wifgb_KYc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved FAISS index\n",
        "vector_store = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
        "# Ask the user for a question about the will\n",
        "query = input(\"‚ùì Ask something about the will: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnaqpwlJJH26",
        "outputId": "128c6f16-0469-4e92-f61c-dd18c6dbd381"
      },
      "execution_count": 73,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùì Ask something about the will: Who is this will about?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for top 3 relevant chunks\n",
        "docs = vector_store.similarity_search(query, k=3)\n",
        "\n",
        "print(\"\\nüîç Top Relevant Chunks:\")\n",
        "for i, doc in enumerate(docs, 1):\n",
        "    print(f\"\\n--- Chunk #{i} ---\\n{doc.page_content}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N1E8fwtK6p0",
        "outputId": "1bea722c-a70a-449f-cf7c-89967df659ed"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Top Relevant Chunks:\n",
            "\n",
            "--- Chunk #1 ---\n",
            "upon all affected. \n",
            "VII. CONTESTING BENEFICIARY \n",
            "If any beneficiary under this Will, or any trust herein mentioned, contests or attacks this \n",
            "Will or any of its provisions, any share or interest in my estate given to that contesting\n",
            "\n",
            "--- Chunk #2 ---\n",
            "beneficiary under this Will is revoked and shall be disposed of in the same manner \n",
            "provided herein as if that contesting beneficiary had predeceased me. \n",
            "VIII. GUARDIAN AD LITEM NOT REQUIRED \n",
            "I direct that the representation by a guardian ad litem of the interests of persons unborn, \n",
            "unascertained or legally incompetent to act in proceedings for the allowance of \n",
            "accounts hereunder be dispensed with to the extent permitted by law. \n",
            "IX. GENDER\n",
            "\n",
            "--- Chunk #3 ---\n",
            "and all other wills and codicils heretofore made by me. \n",
            "I. EXPENSES & TAXES \n",
            "I direct that all my debts, and expenses of my last illness, funeral, and burial, be paid as \n",
            "soon after my death as may be reasonably convenient, and I hereby authorize my \n",
            "Personal Representative, hereinafter appointed, to settle and discharge, in his or her \n",
            "absolute discretion, any claims made against my estate. \n",
            "I further direct that my Personal Representative shall pay out of my estate any and all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 4: Generate Answers using Retrieved Chunks"
      ],
      "metadata": {
        "id": "nos_mWnZLlHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take the top chunks we retrieved from FAISS and feed them into a local language model, like GPT4All, to generate a natural-language answer.\n",
        "\n",
        "This is where retrieval + generation = RAG"
      ],
      "metadata": {
        "id": "qvIWj-A2Lomr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "Dh8TQhG4Mmlu"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch"
      ],
      "metadata": {
        "id": "jW56G1kYLElm"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n"
      ],
      "metadata": {
        "id": "VreJc9NFLr3G"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "response = pipe(\"Summarize this will: \" + docs[0].page_content, max_new_tokens=200)\n",
        "print(response[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VpV-G9bL-Ns",
        "outputId": "cf6abfde-bafd-407a-ee39-230f3cbc8394"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a beneficiary under this Will, or any trust herein mentioned, shall be forfeited to that contesting beneficiary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PjsJXHlmOE40"
      },
      "execution_count": 78,
      "outputs": []
    }
  ]
}